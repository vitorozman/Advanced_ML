{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vajeN_podatki import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odkrivanje enačb, vaje 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. del: linearna regresija\n",
    "\n",
    "Ena najpreprostejših in pogosto precej učinkovitih metod za odkrivanje enačb je redka linearna regresija. Omejena je na enačbe, linearne v parametrih, omogoča nam pa dodajanje členov s poljubnimi oblikami funkcij. Pri linearni regresiji iščemo model oblike $\\hat{y}_i = \\sum\\limits_{k=0}^K \\beta_k X_{ik}$, kjer je $\\vec{\\beta}$ vektor koeficientov, $X$ pa matrika podatkov. Če minimiziramo kvadratno napako $\\sum_i (\\hat{y}_i - y_i)^2$, se da izpeljati analitično rešitev za koeficiente:\n",
    "\n",
    "$$\\vec{\\beta} = (X^TX)^{-1}X^T \\vec{y}$$\n",
    "\n",
    "Pri uporabi linearne regresije za odkrivanje enačb so ključne dodatne spremenljivke, ki jih zgeneriramo. To so lahko višji redi spremenljivk, produkti spremenljivk, logaritmi, trigonometrične funkcije, itd. \n",
    "\n",
    "Za razliko od navadnega strojnega učenja nas pri odkrivanju enačb ne zanimajo preveč napovedi, temveč sam model - najboljša enačba, ki smo jo uspeli odkriti. Ker želimo čim bolj razumljive enačbe, je dobro iz končne enačbe odstraniti člene z zelo majhnimi koeficienti. Berljivost enačbe lahko dodatno izboljšamo tako, da v stringu vrednosti koeficientov zaokrožimo na manjše število decimalnih mest.\n",
    "\n",
    "1.1 Napiši funkcijo za linearno regresijo. Sprejme naj:\n",
    "- X: tabela z N vrsticami, ki ustrezajo N učnim primerom, ter K stolpci, ki ustrezajo K spremenljivkam (v splošnem bodo to originalne spremenljivke plus neko število dodatnih členov), imena stolpcev pa povejo, za kateri člen gre (np. \"log(x-y)\")\n",
    "- y: vektor z N vrednostmi, ki ustrezajo levi strani enačbe.\n",
    "- meja: opcijski argument, ki pove, pod katero vrednostjo koeficienta člen odstranimo iz enačbe.\n",
    "\n",
    "Funkcija naj vrne:\n",
    "- najdeno enačbo v obliki stringa (npr. \"0.32 x + 0.10 x**2 - 2.02 sin(y)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearna_regresija(X, y, meja=1e-2):\n",
    "    imena = X.columns\n",
    "    beta = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "    izraz = \"\"\n",
    "    for i,b in enumerate(beta):\n",
    "        if b > meja:\n",
    "            if len(izraz) > 0:\n",
    "                izraz += \" + \"\n",
    "            izraz +=  f\"{b:.3f}*{imena[i]}\"\n",
    "    return izraz\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Preizkusi svojo funkcijo za linearno regresijo na primeru odkrivanja energijskega zakona $E = mgh + \\frac{1}{2}mv^2$. Za generiranje podatkov lahko uporabiš funkcijo **generiraj_energijski_zakon** iz datoteke **vajeED_1_podatki.py**. Podaš željeno število primerov (recimo 100) ter željeno stopnjo multiplikativnega šuma.\n",
    "\n",
    "Zgenerirati bo potrebno tudi dodatne člene. To lahko sprogramiraš ročno, lahko pa uporabiš kakšno od funkcij iz scikit-learn. Za generiranje polinomskih členov je uporabna **sklearn.preprocessing.PolynomialFeatures**.\n",
    "\n",
    "Kako dobro deluje navadna linearna regresija? Preizkusi vsaj tri nivoje šuma - smiselne vrednosti so recimo 0, 0.001, 0.01 in 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "podatki_energija = generiraj_energijski_zakon(100, sum=0.01)\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "X = poly.fit_transform(podatki_energija.drop(\"E\", axis=1))\n",
    "\n",
    "imena_stolpcev = poly.get_feature_names_out()\n",
    "X_df = pd.DataFrame(X, columns=imena_stolpcev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.265*m + 9.570*m h + 0.403*h^2 + 0.176*h v + 0.208*v^2 + 0.259*m^3 + 0.095*m^2 h + 0.184*m^2 v + 0.092*m h^2 + 0.205*m h v + 0.351*m v^2'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearna_regresija(X_df, podatki_energija[\"E\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Delovanje pri zašumljenih podatkih lahko včasih izboljšamo z regularizacijo. \n",
    "Pri redki (ang. sparse) regresiji želimo obržati vrednosti koeficientov nizke. Visoke vrednosti kaznujemo tako, da v funkcijo napake dodamo regularizacijski člen (ang. ridge regression):  $\\sum_i (\\hat{y}_i - y_i)^2 + \\lambda \\sum_k \\beta_k^2$, kjer parameter $\\lambda$ določa nivo regularizacije. Optimizacijski problem se da še vedno rešiti analitično:\n",
    "\n",
    "$$ \\vec{\\beta} = (X^TX + \\lambda I)^{-1}X^T \\vec{y} $$ \n",
    "\n",
    "Napiši funkcijo za ridge regresijo. Sprejme naj:\n",
    "- X: tabela z N vrsticami, ki ustrezajo N učnim primerom, ter K stolpci, ki ustrezajo K spremenljivkam (v splošnem bodo to originalne spremenljivke plus neko število dodatnih členov), imena stolpcev pa povejo, za kateri člen gre (np. \"log(x-y)\")\n",
    "- y: vektor z N vrednostmi, ki ustrezajo levi strani enačbe.\n",
    "- lambda: regularizacijski parameter, dobro vrednost za dani problem je treba določiti empirično (s poskušanjem)\n",
    "- meja: opcijski argument, ki pove, pod katero vrednostjo koeficienta člen odstranimo iz enačbe\n",
    "\n",
    "\n",
    "Funkcija naj vrne:\n",
    "- najdeno enačbo v obliki stringa (npr. \"0.32 x + 0.10 x**2 - 2.02 sin(y)\")\n",
    "\n",
    "Funkcijo za ridge regresijo preveri na podatkih za energijski zakon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regresija(X, y, lam=1, meja=1e-2):\n",
    "    imena = X.columns\n",
    "    beta = np.linalg.pinv(X.T.dot(X) + lam*np.identity(X.shape[1])).dot(X.T).dot(y)\n",
    "\n",
    "    izraz = \"\"\n",
    "    for i,b in enumerate(beta):\n",
    "        if b > meja:\n",
    "            if len(izraz) > 0:\n",
    "                izraz += \" + \"\n",
    "            izraz +=  f\"{b:.3f}*{imena[i]}\"\n",
    "    return izraz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.171*m + 1.172*h + 0.342*m^2 + 2.945*m h + 0.322*m v + 0.398*h^2 + 0.061*h v + 2.225*m^2 h + 2.142*m h^2 + 0.919*m h v + 0.210*m v^2 + 0.022*v^3'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "podatki_energija = generiraj_energijski_zakon(100, sum=0.01)\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "X = poly.fit_transform(podatki_energija.drop(\"E\", axis=1))\n",
    "\n",
    "imena_stolpcev = poly.get_feature_names_out()\n",
    "X_df = pd.DataFrame(X, columns=imena_stolpcev)\n",
    "\n",
    "ridge_regresija(X_df, podatki_energija[\"E\"], lam=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Ridge regresija za ta problem očitno ne deluje najboljše. V resnici je za odkrivnaje enačb bolj primerna regularizacija z normo L1:\n",
    "$\\sum_i (\\hat{y}_i - y_i)^2 + \\lambda \\sum_k |\\beta_k|$.\n",
    "Taki linearni regresiji rečemo Lasso in žal nima analitične rešitve, zato moramo optimalne vrednosti parametrov $\\beta_k$ iskati z numerično minimizacijo (recimo **scipy.optimize.minimize**).\n",
    "\n",
    "Napiši funkijo za Lasso regresijo z istimi vhodi in izhodi kot v 1.3 ter jo preizkusi na podatkih za energijski zakon. Ali deluje bo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def lasso_regresija(X, y, lam=1, meja=1e-2):\n",
    "    imena = X.columns\n",
    "\n",
    "    def f(beta):\n",
    "        yhat = X.dot(beta)\n",
    "        return np.sum((yhat-y)**2) + lam*np.sum(np.abs(beta))\n",
    "    beta = minimize(f, np.random.random(X.shape[1]))[\"x\"]\n",
    "    \n",
    "    izraz = \"\"\n",
    "    for i,b in enumerate(beta):\n",
    "        if b > meja:\n",
    "            if len(izraz) > 0:\n",
    "                izraz += \" + \"\n",
    "            izraz +=  f\"{b:.3f}*{imena[i]}\"\n",
    "    return izraz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9.732*m h + 0.348*m v + 0.022*m^2 v + 0.030*m v^2 + 0.016*v^3'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podatki_energija = generiraj_energijski_zakon(100, sum=0.01)\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "X = poly.fit_transform(podatki_energija.drop(\"E\", axis=1))\n",
    "\n",
    "imena_stolpcev = poly.get_feature_names_out()\n",
    "X_df = pd.DataFrame(X, columns=imena_stolpcev)\n",
    "\n",
    "lasso_regresija(X_df, podatki_energija[\"E\"], lam=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DODATNO: Odkrivanje enačb z linearno regresijo lahko preizkusiš tudi na drugih podatkih v datoteki **vajeED_1_podatki.py**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DODATNO-2: Na vajah smo spoznali osnovno implementacijo redke regresije za odkrivanje enačb. Za bolj resne poskuse si lahko pogledaš knjižnico SINDy, ki je namenjena predvsem odkrivanju sistemov diferencialnih enačb, ki opisujejo dinamične sisteme: https://github.com/dynamicslab/pysindy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. del - BACON\n",
    "Na predavanjih smo spoznali enega prvih algoritmov za odkrivanje enačb - BACON. Ta algoritem je koristen za odkrivanje enačb, ki vključujejo samo množenje in deljenje. \n",
    "\n",
    "![algoritem BACON](bacon.png)\n",
    "\n",
    "Zapiši funkcijo, ki sprejme tabelo T s stolpci $x_i$, $1 \\le i \\le m$ in najde zvezo $c = E(x_1, \\dots , x_m)$ po Baconovi metodi (alg. 1). Premisli, kako poimenovati nove spremenljivke in ali se da pogoja iz druge in tretje veje if-stavka malo razrahljati.\n",
    "\n",
    "Metodo preizkusi na Newtonovem zakonu $F = ma$ (**generiraj_newton**) in Stefanovem zakonu $j = \\sigma T^4$ (**generiraj_stefan**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bacon(df, max_iter=20):\n",
    "    for iteracija in range(max_iter):\n",
    "        stolpci = list(df.columns)\n",
    "        df_array = np.array(df)\n",
    "        # ocenimo, ali obstaja konstanta\n",
    "        sig = np.std(df_array, axis=0)\n",
    "        # testirajmo se trivialnost\n",
    "        if min(sig) < 10 ** -12:  # pazimo na skalo?\n",
    "            break\n",
    "        # izracunajmo vrstne rede, najdemo korelacije med njimi\n",
    "        vrstni_redi = np.array(df.rank(axis=0)).T\n",
    "        korelacije = np.corrcoef(vrstni_redi)  # korelacije[i, j]\n",
    "        n = korelacije.shape[0]\n",
    "        korelacije = [(abs(korelacije[i, j]), (i, j), korelacije[i, j] < 0) for i in range(n) for j in range(i + 1, n)]\n",
    "        korelacije.sort(reverse=True)\n",
    "        for kakovost, (i, j), je_mnozenje in korelacije:\n",
    "            if je_mnozenje:\n",
    "                ime_novega = f\"({stolpci[i]}) * ({stolpci[j]})\"\n",
    "                vrednosti_novega = df_array[:, i] * df_array[:, j]\n",
    "            else:\n",
    "                ime_novega = f\"({stolpci[i]}) / ({stolpci[j]})\"\n",
    "                vrednosti_novega = df_array[:, i] / df_array[:, j]\n",
    "            if ime_novega not in stolpci:\n",
    "                df[ime_novega] = vrednosti_novega\n",
    "                break\n",
    "    # najdemo \"konstanto\"\n",
    "    df_array = np.array(df)\n",
    "    sig = np.std(df_array, axis=0)\n",
    "    i = np.argmin(sig)\n",
    "    const = np.mean(df_array[:, i])\n",
    "    print(f\"{const:.5e} = {df.columns[i]} (napaka: {sig[i]})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "podatki = generiraj_newton(100, sum=0)\n",
    "bacon(podatki)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DODATNO: Če te odkrivanje enačb zanima, si lahko pogledaš še modernejše algoritme. \n",
    "\n",
    "**pySR** je enostavna za instalacijo, temelji pa na genetskih algoritmih: https://github.com/MilesCranmer/PySR\n",
    "\n",
    "**DSO** je trenutno ena od najmočnejših metod in združuje globoko učenje ter genetske algoritme: https://github.com/brendenpetersen/deep-symbolic-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
